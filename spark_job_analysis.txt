Spark Job Failure Analysis: Resource Starvation at Application StartupExecutive SummaryA Spark application failed with multiple jobs showing "Stage cancelled because SparkContext was shut down" after only 9-10 seconds of execution. The root cause was severe executor resource starvation at application startup, not shuffle write errors or data processing issues. Tasks were unable to execute because insufficient executor resources were available.


Problem DescriptionObserved SymptomsFailed Jobs:

Job 1: insertInto at HiveWriteComponent.java:163 - 12 tasks (1 succeeded, 11 stuck)
Job 15: insertInto at HiveWriteComponent.java:163 - 2 tasks (0 succeeded, 2 stuck)
Job 19: insertInto at HiveWriteComponent.java:163 - Similar pattern to Job 15
Job 42 (Stage 52): broadcast exchange operation - Failed with thread execution exception
Key Indicators:

Total execution time before failure: 9-10 seconds
Tasks stuck in "RUNNING" state but making no progress
No shuffle write metrics shown for stuck tasks
Failure reason: "Stage cancelled because SparkContext was shut down"
HTTP 403 errors when attempting to access task logs
Executors added later (3, 4, 5, 7, 0) after jobs already failed

Timeline of Events

01:56:05 - Job 1, Stage 1 starts (12 tasks needed)
01:56:06 - Job 15, Stage 15 starts (2 tasks needed)
         - Only 1-2 executor slots available total
         - Task 3 from Stage 1 completes (got lucky, found a slot)
         - All other tasks stuck in queue waiting for executor resources
         
01:56:14 - No progress made, tasks still queued
         - Dynamic allocation may start detecting backlog
         
01:56:15 - SparkContext shutdown triggered (timeout/watchdog)
         - All stages cancelled
         - Jobs marked as failed
         
01:56:20+ - New executors arrive (too late)
          - Application retries or new attempt succeeds
		  
Root Cause Analysis
Primary Root Cause: Insufficient Executor Resources at Startup
Default Spark Configuration (likely in use):

spark.executor.instances = 2              # Only 2 executors by default
spark.executor.cores = 1                  # 1 core per executor (YARN default)
Total available task slots = 2 × 1 = 2
```

**Application Requirements:**
```
Stage 1: 12 tasks (needs 12 slots)
Stage 15: 2 tasks (needs 2 slots)
Stage 19: 1-2 tasks (needs 1-2 slots)
Multiple stages running concurrently
```

**The Problem:**
With only 2 task slots available but 15+ tasks needing to run across multiple concurrent stages, tasks were queued indefinitely waiting for resources that didn't exist.

### Why Tasks Appeared "RUNNING" But Made No Progress

Tasks were:
1. Submitted to the Spark scheduler
2. Marked as "RUNNING" in the scheduler's queue
3. **Never actually executed on any executor** (no slots available)
4. Stuck waiting for executor resources
5. Cancelled when SparkContext shut down

This explains why:
- Stage 15 with only 2 tasks had 0/2 completion (couldn't even start)
- Stage 1 had 1/12 completion (one task got lucky and found a slot)
- No shuffle write metrics appeared (tasks never reached execution phase)

### Secondary Factors

**1. Dynamic Allocation Too Slow**
If dynamic allocation was enabled, it failed to provision executors quickly enough:
```
Problem: Executor provisioning (5-15 seconds) > Application timeout (9-10 seconds)
Result: New executors arrived after SparkContext already shut down		  




spark job analysis

----------------------------------------
Stage 1 Overview (Image 1):

Total of 12 tasks in this stage
Input: 60.9 MiB / 273,134 records
Shuffle Write: 162.9 MiB / 273,134 records
Only 10 seconds total time across all tasks

The Problem Pattern (Images 4-6):

Task 3 (Index 1): SUCCESS - Completed in 10s, wrote all its shuffle data (162.9 MiB / 273,134 records)
Tasks 2 and 52 (Index 0, 2): RUNNING - Stuck with no progress, no shuffle write data shown
11 other tasks also stuck in RUNNING state


What Likely Happened
1. Shuffle Write Bottleneck
The successful task wrote 162.9 MiB of shuffle data, but the 11 running tasks show no shuffle write metrics at all. This strongly suggests:

Tasks started processing their input data
They got to the shuffle write phase
The shuffle write operation hung or blocked - couldn't write to disk or the shuffle service
Tasks remained in "RUNNING" state but made no progress

2. Possible Root Causes
Given the symptoms and the fact that adding executors helped:
Disk I/O Saturation:


Executor 1 had too many tasks trying to write shuffle data simultaneously
→ Disk became bottleneck (too many concurrent writes)
→ Tasks blocked waiting for disk I/O
→ Only 1 task succeeded, 11 got stuck


External shuffle service on that node was overwhelmed
→ Tasks couldn't register their shuffle outputs
→ Blocked indefinitely


Insufficient memory to buffer shuffle data before writing
→ Tasks waiting for memory to become available
→ One task grabbed resources, others starved


Why Adding Executors Helped
From your timeline screenshot, after executors were added:
Before (single executor):

Executor 1: [Task 3 ✓] [Task 2 stuck] [Task 52 stuck] ... (9 more stuck)
            └─ All 12 tasks competing for same resources
			
			
Executor 1: [Task 3 ✓] [Tasks stuck/killed]
Executor 3: [New tasks succeed]
Executor 4: [New tasks succeed]
Executor 5: [New tasks succeed]
...


The Shuffle Write Size Mystery
Why is shuffle write (162.9 MiB) larger than input (60.9 MiB)?
This is actually normal for Hive writes and suggests:

Data expansion during serialization - converting in-memory objects to on-disk format
Metadata overhead - Hive partition information, schema data
Compression differences - input might be compressed differently than shuffle output
Dynamic partitioning - writing to multiple partitions creates replication of metadata

Summary Metrics (Image 3)
The fact that only 1 task completed means:

Min/25th/Median/75th/Max all show identical values (60.9 MiB input, 162.9 MiB shuffle write)
This is because there's only ONE data point - the single successful task
The 11 stuck tasks contributed no metrics

What This Tells You
Your hypothesis is correct:

Shuffle write was the bottleneck - tasks got stuck trying to write shuffle data
Resource exhaustion on initial executor - likely disk I/O or shuffle service overload
Dynamic allocation saved the job - adding executors provided fresh resources
Task retry mechanism - Spark likely killed the stuck tasks and re-ran them on new executors


Recommendations
To prevent this in the future:

Increase spark.executor.instances to avoid overloading single executors
Tune shuffle parameters: spark.shuffle.file.buffer (default 32KB), spark.reducer.maxSizeInFlight
Monitor disk I/O on executors during shuffle-heavy operations
Consider SSD storage for shuffle directories if using spinning disks
Increase spark.task.maxFailures to allow more retries before job failure


-------------------------------------------------------


Recommendations
To prevent this in the future:

Increase spark.executor.instances to avoid overloading single executors
Tune shuffle parameters: spark.shuffle.file.buffer (default 32KB), spark.reducer.maxSizeInFlight
Monitor disk I/O on executors during shuffle-heavy operations
Consider SSD storage for shuffle directories if using spinning disks
Increase spark.task.maxFailures to allow more retries before job failure

This is a textbook example of resource contention during shuffle operations!


------------------------------------------------------


tasks timeline

Task Execution Pattern Analysis
Visual Breakdown by Task Rows:
Row 1 (Top task):

Entirely bright green (Executor Computing Time)
Full width, completed quickly
This is the successful task that finished

Row 2 (Second task):

Small red/pink segment at start (Task Deserialization Time)
Long bright green segment (Executor Computing Time)
Tiny yellow segment at end (Shuffle Write Time)
Small purple segment at very end (Result Serialization Time)
This task also appears to have completed!

Rows 3-4:

Start later (gap before they begin)
Dominated by bright green (Executor Computing Time)
These appear to complete as well

Rows 5-12 (Bottom half):

All start even later (significant gap)
All show bright green extending to the right edge
These are the tasks that were likely still running or retried on new executors

Key Insights
1. Staggered Start Times
Unlike a typical well-parallelized stage where all tasks start simultaneously, these tasks show three distinct waves:

Wave 1: Tasks 1-2 start immediately
Wave 2: Tasks 3-4 start after a delay
Wave 3: Tasks 5-12 start much later

This pattern suggests:

Limited executor slots initially - couldn't run all 12 tasks at once
Task scheduling delays - had to wait for slots to become available
Executor addition - the later tasks likely ran on newly added executors

2. The "Black Bar" at Bottom
That thick black/dark segment near the bottom appears to be:

A timeline marker or separator
Possibly indicating when executors were added
Or marking a failure/retry boundary

3. No Visible Shuffle Write for Most Tasks
Most tasks show:

Dominated by green (computing)
Very little or no visible yellow (shuffle write)
This could mean:

Shuffle write was extremely fast (optimistic)
Shuffle write is included in computing time (likely)
Tasks didn't reach shuffle write phase (for the stuck ones)



Reconciling with Previous Screenshots
From your earlier screenshots, we saw:

12 tasks total
1 succeeded
11 were "RUNNING"

But this timeline suggests more tasks may have succeeded. Here's why the discrepancy:
Possible Explanations:

Snapshot timing - The task table screenshots were taken mid-execution when 11 tasks were still running
This timeline might be from a retry - After the initial failure, the stage was re-executed with better resource allocation
Different attempt - This could be "Attempt 1" after "Attempt 0" failed

What the Timeline Really Shows
Healthy Task Execution:

Row 1: Quick, clean execution (~10s based on your metrics)
Row 2: Normal task lifecycle with visible phases
Rows 3-4: Clean execution, just started later

Resource-Constrained Execution:

Rows 5-12: These tasks couldn't start until much later
They're all computing simultaneously (parallel green bars)
This indicates executor resources finally became available


Time →

[Exec 1] Task 1 ████████ (completes)
         Task 2 ▓▓████████▓ (completes)
         
         ⏸️ (waiting for executor slots...)
         
[Exec 1] Task 3      ████████ (completes)  
[Exec 3] Task 4      ████████ (completes)

         ⏸️ (more executors added...)
         
[Exec 1] Task 5              ████████████████
[Exec 3] Task 6              ████████████████
[Exec 4] Task 7              ████████████████
[Exec 5] Task 8              ████████████████
...      Tasks 9-12          ████████████████





The Actual Failure Reason
Failure Reason: "Stage cancelled because SparkContext was shut down"
This is a critical finding that completely changes our understanding of what happened.
What This Actually Means
SparkContext shutdown is not a normal task failure—it means the entire Spark application was terminated. This can happen for several reasons:
1. Application-Level Timeout or Kill Signal
The most likely scenario:



Stage 52 (and the job containing it):
1. Started execution at 01:56:22
2. Had 1 task that started but never completed (0/1 succeeded, 1 running)
3. Task got stuck (likely during shuffle write as we analyzed)
4. Application-level timeout triggered
5. SparkContext shut down
6. Stage 52 marked as "cancelled"



Timeline:

01:56:05 - Job 1, Stage 1 starts
         - 12 tasks created
         - Task 3 completes successfully (10s)
         - Tasks 2, 52, and 9 others get stuck

01:56:14 - Task 3 finishes, others still stuck

01:56:22 - Stage 52 starts (related to Job 42)
         - broadcast exchange operation
         - 1 task starts

~01:56:25 - Spark detects tasks haven't made progress
          - Could be:
            * spark.task.timeout exceeded
            * Executor heartbeat timeout
            * Driver-executor communication failure
          
~01:56:25 - SparkContext initiates shutdown
          - All running stages cancelled
          - Stage 52 shows: "cancelled because SparkContext was shut down"
          - Job 42, Job 1, etc. marked as failed
          
Later     - Dynamic allocation adds new executors (3, 4, 5, 7, 0)
          - Application attempts recovery or retries
          - Eventually succeeds
		  

Root Cause: Severe Executor Resource Starvation
What happened:

Initial State: Only 1 executor with 1-2 cores available
Stage 1 starts: Needs 12 task slots, gets 1-2
One task runs and completes: Uses the available slot
11 tasks queue up: Waiting for slots
Stage 15 starts: Needs 2 more task slots
No slots available: All blocked by Stage 1's queued tasks
Stage 19 starts: Same problem
Deadlock/starvation: Multiple stages competing for non-existent resources
9-10 seconds pass: No progress made
SparkContext shutdown: Watchdog kills the application

# 1. Start with sufficient executors immediately
spark.dynamicAllocation.enabled = true
spark.dynamicAllocation.initialExecutors = 4
spark.dynamicAllocation.minExecutors = 2
spark.dynamicAllocation.maxExecutors = 10

# 2. Request executors faster when needed
spark.dynamicAllocation.schedulerBacklogTimeout = 1s  # Default is 1s, but ensure it's set
spark.dynamicAllocation.executorIdleTimeout = 60s

# 3. OR just use static allocation with enough executors
spark.dynamicAllocation.enabled = false
spark.executor.instances = 6  # Enough for 12+ tasks

# 4. Ensure each executor can run multiple tasks
spark.executor.cores = 4  # Each executor can run 4 tasks simultaneously
spark.task.cpus = 1  # Each task needs 1 core

# With 6 executors × 4 cores = 24 task slots
# More than enough for your 12 tasks in Stage 1 + 2 in Stage 15
```

## Why Dynamic Allocation Failed You

From your timeline, executors were eventually added (3, 4, 5, 7, 0), but they came **too late**:
```
Timeline:
01:56:05 - Stages start, need resources
01:56:06 - No resources available
01:56:10 - Still waiting...
01:56:14 - Dynamic allocation detects backlog
01:56:15 - Requests more executors
01:56:15 - SparkContext shuts down (timeout!)
01:56:20 - New executors arrive (too late)